{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, get accuracy for persistance forecast and WG forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "''' Import relevant libraries '''\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all wind data after filtering for direction and speed\n",
    "\n",
    "# cols are: [0 Date, 1 chimet speed, 2 chimet dir, 3 WG speed, 4 Wg dir]\n",
    "\n",
    "data_SW10 = np.load('data_SW10.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.195358878082382\n"
     ]
    }
   ],
   "source": [
    "# Calculate WG rmse\n",
    "WG_rmse = mean_squared_error(data_SW10[:,-12:,1],data_SW10[:,-12:,3])**(1/2)\n",
    "print(WG_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5565203973061905\n"
     ]
    }
   ],
   "source": [
    "# Creat persistance column\n",
    "persistance=data_SW10\n",
    "for i in range(0,data_SW10.shape[0]):\n",
    "    persistance[i,:,2]=np.ones(48)*data_SW10[i,35,1]\n",
    "    \n",
    "np.delete(persistance,3,axis=2)\n",
    "np.delete(persistance,4,axis=2)\n",
    "\n",
    "prst_rmse = mean_squared_error(persistance[:,-12:,1],persistance[:,-12:,2])**(1/2)\n",
    "print(prst_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_SW10.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' fit an LSTM network to training data '''\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons, dropout=0.5):\n",
    "    \n",
    "    # make training and validation set lengths divisble by batch num\n",
    "    len_train = np.floor(train.shape[0]*0.5/n_batch)*n_batch*2\n",
    "    len_train = len_train.astype(int)\n",
    "    print(len_train)\n",
    "    \n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:len_train, 0:n_lag], train[:len_train, n_lag:]\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True, recurrent_dropout=dropout))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # define restart schedule\n",
    "    schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                        max_lr=1e-2,\n",
    "                        steps_per_epoch=train.shape[0],\n",
    "                        lr_decay=0.9,\n",
    "                        cycle_length=5,\n",
    "                        mult_factor=1.5)\n",
    "    \n",
    "    # fit network\n",
    "    #model.fit(X, y, epochs=nb_epoch, batch_size=n_batch, shuffle=True, verbose=1, validation_split=0.5)\n",
    "    model.fit(X, y, epochs=nb_epoch, batch_size=n_batch, shuffle=True, verbose=1, validation_split=1,callbacks=[schedule])\n",
    "    #model.reset_states()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16721, 48, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_SW10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5376\n",
      "Epoch 1/50\n",
      "5376/5376 [==============================] - 2s 444us/step - loss: 50.9202\n",
      "Epoch 2/50\n",
      "5376/5376 [==============================] - 2s 364us/step - loss: 27.6730\n",
      "Epoch 3/50\n",
      "5376/5376 [==============================] - 2s 386us/step - loss: 26.3747\n",
      "Epoch 4/50\n",
      "5376/5376 [==============================] - 2s 394us/step - loss: 26.3005\n",
      "Epoch 5/50\n",
      "5376/5376 [==============================] - 2s 415us/step - loss: 26.1529\n",
      "Epoch 6/50\n",
      "5376/5376 [==============================] - 2s 381us/step - loss: 25.6938\n",
      "Epoch 7/50\n",
      "5376/5376 [==============================] - 2s 394us/step - loss: 25.3558\n",
      "Epoch 8/50\n",
      "5376/5376 [==============================] - 2s 392us/step - loss: 25.3453\n",
      "Epoch 9/50\n",
      "5376/5376 [==============================] - 2s 398us/step - loss: 25.0430\n",
      "Epoch 10/50\n",
      "5376/5376 [==============================] - 2s 398us/step - loss: 24.9411\n",
      "Epoch 11/50\n",
      "5376/5376 [==============================] - 3s 469us/step - loss: 24.9465\n",
      "Epoch 12/50\n",
      "5376/5376 [==============================] - 2s 340us/step - loss: 24.8918\n",
      "Epoch 13/50\n",
      "5376/5376 [==============================] - 2s 344us/step - loss: 24.8177\n",
      "Epoch 14/50\n",
      "5376/5376 [==============================] - 2s 360us/step - loss: 24.6853\n",
      "Epoch 15/50\n",
      "5376/5376 [==============================] - 2s 415us/step - loss: 24.63460s - loss:\n",
      "Epoch 16/50\n",
      "5376/5376 [==============================] - 3s 480us/step - loss: 24.6459\n",
      "Epoch 17/50\n",
      "5376/5376 [==============================] - 2s 375us/step - loss: 24.6460\n",
      "Epoch 18/50\n",
      "5376/5376 [==============================] - 2s 362us/step - loss: 24.6062\n",
      "Epoch 19/50\n",
      "5376/5376 [==============================] - 2s 372us/step - loss: 24.6507\n",
      "Epoch 20/50\n",
      "5376/5376 [==============================] - 2s 401us/step - loss: 24.6029\n",
      "Epoch 21/50\n",
      "5376/5376 [==============================] - 2s 395us/step - loss: 24.6341\n",
      "Epoch 22/50\n",
      "5376/5376 [==============================] - 2s 403us/step - loss: 24.5834\n",
      "Epoch 23/50\n",
      "5376/5376 [==============================] - 2s 412us/step - loss: 24.4667\n",
      "Epoch 24/50\n",
      "5376/5376 [==============================] - 2s 422us/step - loss: 24.5579\n",
      "Epoch 25/50\n",
      "5376/5376 [==============================] - 2s 422us/step - loss: 24.6176\n",
      "Epoch 26/50\n",
      "5376/5376 [==============================] - 2s 425us/step - loss: 24.5345\n",
      "Epoch 27/50\n",
      "5376/5376 [==============================] - 2s 436us/step - loss: 24.5253\n",
      "Epoch 28/50\n",
      "5376/5376 [==============================] - 2s 430us/step - loss: 24.5272\n",
      "Epoch 29/50\n",
      "5376/5376 [==============================] - 2s 446us/step - loss: 24.4692\n",
      "Epoch 30/50\n",
      "5376/5376 [==============================] - 2s 420us/step - loss: 24.4410\n",
      "Epoch 31/50\n",
      "5376/5376 [==============================] - 2s 450us/step - loss: 24.4310\n",
      "Epoch 32/50\n",
      "5376/5376 [==============================] - 2s 445us/step - loss: 24.4995\n",
      "Epoch 33/50\n",
      "5376/5376 [==============================] - 2s 422us/step - loss: 24.5886\n",
      "Epoch 34/50\n",
      "5376/5376 [==============================] - 2s 449us/step - loss: 24.5885\n",
      "Epoch 35/50\n",
      "5376/5376 [==============================] - 3s 477us/step - loss: 24.5017\n",
      "Epoch 36/50\n",
      "5376/5376 [==============================] - 3s 474us/step - loss: 24.5163\n",
      "Epoch 37/50\n",
      "5376/5376 [==============================] - 3s 467us/step - loss: 24.4257\n",
      "Epoch 38/50\n",
      "5376/5376 [==============================] - 2s 429us/step - loss: 24.4932\n",
      "Epoch 39/50\n",
      "5376/5376 [==============================] - 2s 460us/step - loss: 24.4124\n",
      "Epoch 40/50\n",
      "5376/5376 [==============================] - 2s 455us/step - loss: 24.5395\n",
      "Epoch 41/50\n",
      "5376/5376 [==============================] - 3s 476us/step - loss: 24.4793\n",
      "Epoch 42/50\n",
      "5376/5376 [==============================] - 2s 459us/step - loss: 24.5299\n",
      "Epoch 43/50\n",
      "5376/5376 [==============================] - 3s 468us/step - loss: 24.6098\n",
      "Epoch 44/50\n",
      "5376/5376 [==============================] - 3s 542us/step - loss: 24.4826\n",
      "Epoch 45/50\n",
      "5376/5376 [==============================] - 3s 495us/step - loss: 24.4412\n",
      "Epoch 46/50\n",
      "5376/5376 [==============================] - 2s 424us/step - loss: 24.4531\n",
      "Epoch 47/50\n",
      "5376/5376 [==============================] - 3s 495us/step - loss: 24.5461\n",
      "Epoch 48/50\n",
      "5376/5376 [==============================] - 3s 515us/step - loss: 24.4531\n",
      "Epoch 49/50\n",
      "5376/5376 [==============================] - 3s 497us/step - loss: 24.3586\n",
      "Epoch 50/50\n",
      "5376/5376 [==============================] - 3s 482us/step - loss: 24.3776\n"
     ]
    }
   ],
   "source": [
    "n_lag = 36\n",
    "n_seq = 12\n",
    "nb_epoch = 50\n",
    "n_batch = 32   # batch size must be divisible by number of cores (8 in this case)\n",
    "n_neurons = 32\n",
    "dropout=0.8\n",
    "\n",
    "train=data_SW10[:5408,:,1] #first half of data\n",
    "test1=data_SW10[5408:10816,:,1]\n",
    "test2=data_SW10[10816:16224,:,1]\n",
    "\n",
    "lstm_fit=fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.242188, 14.056882, 13.916418, 13.770002, 13.595895, 13.643635,\n",
       "        13.385409, 13.424304, 13.511431, 13.423963, 13.295616, 13.375128],\n",
       "       [11.792924, 11.826064, 11.863474, 11.862047, 11.84593 , 12.036204,\n",
       "        11.933744, 12.090574, 12.27532 , 12.267679, 12.238291, 12.379164]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_pred = Sequential()\n",
    "lstm_pred.add(LSTM(n_neurons, batch_input_shape=(2, 36, 1), stateful=True, recurrent_dropout=dropout))\n",
    "lstm_pred.add(Dense(12))\n",
    "lstm_pred.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# copy weights\n",
    "old_weights = lstm_fit.get_weights()\n",
    "lstm_pred.set_weights(old_weights)\n",
    "\n",
    "X = test1[:2, 0:n_lag]\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "X.shape\n",
    "lstm_pred.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,169):\n",
    "    X = test1[i*32:(i+1)*32, 0:n_lag]\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    pred=lstm_fit.predict(X)\n",
    "    if i == 0:\n",
    "        pred1=pred\n",
    "    else:\n",
    "        pred1=np.append(pred1,pred,axis=0)\n",
    "\n",
    "X=pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5408, 12)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "X2 = data_SW10[5408:10816,-12:,3]\n",
    "X2 = X2.reshape(X2.shape[0], X2.shape[1], 1)\n",
    "X2.shape\n",
    "X=np.append(X1,X2,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5408/5408 [==============================] - 2s 311us/step - loss: 59.9852\n",
      "Epoch 2/50\n",
      "5408/5408 [==============================] - 1s 213us/step - loss: 32.3317\n",
      "Epoch 3/50\n",
      "5408/5408 [==============================] - 1s 263us/step - loss: 31.0410\n",
      "Epoch 4/50\n",
      "5408/5408 [==============================] - 1s 188us/step - loss: 29.5796\n",
      "Epoch 5/50\n",
      "5408/5408 [==============================] - 1s 194us/step - loss: 29.3440\n",
      "Epoch 6/50\n",
      "5408/5408 [==============================] - 1s 198us/step - loss: 28.2504\n",
      "Epoch 7/50\n",
      "5408/5408 [==============================] - 1s 199us/step - loss: 27.3952\n",
      "Epoch 8/50\n",
      "5408/5408 [==============================] - 1s 194us/step - loss: 26.7013\n",
      "Epoch 9/50\n",
      "5408/5408 [==============================] - 1s 199us/step - loss: 26.1114\n",
      "Epoch 10/50\n",
      "5408/5408 [==============================] - 1s 216us/step - loss: 24.1907\n",
      "Epoch 11/50\n",
      "5408/5408 [==============================] - 1s 222us/step - loss: 23.9415\n",
      "Epoch 12/50\n",
      "5408/5408 [==============================] - 2s 316us/step - loss: 23.1992\n",
      "Epoch 13/50\n",
      "5408/5408 [==============================] - 1s 219us/step - loss: 22.7870\n",
      "Epoch 14/50\n",
      "5408/5408 [==============================] - 1s 269us/step - loss: 22.7350\n",
      "Epoch 15/50\n",
      "5408/5408 [==============================] - 2s 372us/step - loss: 22.2781\n",
      "Epoch 16/50\n",
      "5408/5408 [==============================] - 1s 211us/step - loss: 22.3230\n",
      "Epoch 17/50\n",
      "5408/5408 [==============================] - 1s 205us/step - loss: 22.2432\n",
      "Epoch 18/50\n",
      "5408/5408 [==============================] - 1s 219us/step - loss: 22.2367\n",
      "Epoch 19/50\n",
      "5408/5408 [==============================] - 1s 223us/step - loss: 21.8949\n",
      "Epoch 20/50\n",
      "5408/5408 [==============================] - 1s 230us/step - loss: 22.1825\n",
      "Epoch 21/50\n",
      "5408/5408 [==============================] - 1s 244us/step - loss: 22.1351\n",
      "Epoch 22/50\n",
      "5408/5408 [==============================] - 1s 245us/step - loss: 21.8843\n",
      "Epoch 23/50\n",
      "5408/5408 [==============================] - 1s 238us/step - loss: 21.7509\n",
      "Epoch 24/50\n",
      "5408/5408 [==============================] - 1s 242us/step - loss: 21.8239\n",
      "Epoch 25/50\n",
      "5408/5408 [==============================] - 1s 255us/step - loss: 22.0977\n",
      "Epoch 26/50\n",
      "5408/5408 [==============================] - 1s 244us/step - loss: 21.8118\n",
      "Epoch 27/50\n",
      "5408/5408 [==============================] - ETA: 0s - loss: 21.72 - 1s 238us/step - loss: 21.6577\n",
      "Epoch 28/50\n",
      "5408/5408 [==============================] - 1s 239us/step - loss: 21.6617\n",
      "Epoch 29/50\n",
      "5408/5408 [==============================] - 1s 236us/step - loss: 21.5692\n",
      "Epoch 30/50\n",
      "5408/5408 [==============================] - 1s 235us/step - loss: 21.7824\n",
      "Epoch 31/50\n",
      "5408/5408 [==============================] - 1s 243us/step - loss: 21.5762\n",
      "Epoch 32/50\n",
      "5408/5408 [==============================] - 1s 252us/step - loss: 21.5210\n",
      "Epoch 33/50\n",
      "5408/5408 [==============================] - 1s 236us/step - loss: 21.6130\n",
      "Epoch 34/50\n",
      "5408/5408 [==============================] - 1s 232us/step - loss: 21.8089\n",
      "Epoch 35/50\n",
      "5408/5408 [==============================] - 2s 363us/step - loss: 21.3738\n",
      "Epoch 36/50\n",
      "5408/5408 [==============================] - 1s 233us/step - loss: 21.5956\n",
      "Epoch 37/50\n",
      "5408/5408 [==============================] - 1s 225us/step - loss: 21.4781\n",
      "Epoch 38/50\n",
      "5408/5408 [==============================] - 1s 231us/step - loss: 21.4067\n",
      "Epoch 39/50\n",
      "5408/5408 [==============================] - 1s 233us/step - loss: 21.3859\n",
      "Epoch 40/50\n",
      "5408/5408 [==============================] - 1s 265us/step - loss: 21.4262\n",
      "Epoch 41/50\n",
      "5408/5408 [==============================] - 1s 260us/step - loss: 21.5039\n",
      "Epoch 42/50\n",
      "5408/5408 [==============================] - 1s 228us/step - loss: 21.3649\n",
      "Epoch 43/50\n",
      "5408/5408 [==============================] - 1s 214us/step - loss: 21.5068\n",
      "Epoch 44/50\n",
      "5408/5408 [==============================] - 1s 222us/step - loss: 21.4321\n",
      "Epoch 45/50\n",
      "5408/5408 [==============================] - 1s 230us/step - loss: 21.3128\n",
      "Epoch 46/50\n",
      "5408/5408 [==============================] - 1s 228us/step - loss: 21.2949\n",
      "Epoch 47/50\n",
      "5408/5408 [==============================] - 1s 243us/step - loss: 21.3623\n",
      "Epoch 48/50\n",
      "5408/5408 [==============================] - 1s 238us/step - loss: 21.2893\n",
      "Epoch 49/50\n",
      "5408/5408 [==============================] - 1s 236us/step - loss: 21.1985\n",
      "Epoch 50/50\n",
      "5408/5408 [==============================] - 1s 235us/step - loss: 21.1310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ce5d3df748>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=data_SW10[5408:10816,-12:,1]\n",
    "\n",
    "nb_epoch = 50\n",
    "n_batch = 32   # batch size must be divisible by number of cores (8 in this case)\n",
    "n_neurons = 32\n",
    "dropout=0.8\n",
    "\n",
    "#create level 2 lstm\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True, recurrent_dropout=dropout))\n",
    "model.add(Dense(y.shape[1]))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                    max_lr=1e-2,\n",
    "                    steps_per_epoch=train.shape[0],\n",
    "                    lr_decay=0.9,\n",
    "                    cycle_length=5,\n",
    "                    mult_factor=1.5)\n",
    "\n",
    "''' Fit model '''\n",
    "model.fit(X, y, epochs=nb_epoch, batch_size=n_batch, shuffle=True, verbose=1, validation_split=1,callbacks=[schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5408, 36)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
